{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "MSqmEfzv_VFU",
        "outputId": "108797ce-3f15-4edb-bae0-94a6586625ad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3073205999.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# !pip install einops -q # ViT 구현에 필요한 einops 라이브러리 설치\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os, math, time, json, warnings, random\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from contextlib import nullcontext\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- Pretty table printing & optional saving ---\n",
        "PRINT_MAX_ROWS = 30\n",
        "\n",
        "try:\n",
        "    from rich.console import Console\n",
        "    from rich.table import Table\n",
        "    _RICH = True\n",
        "    _console = Console()\n",
        "except Exception:\n",
        "    _RICH = False\n",
        "    _console = None\n",
        "\n",
        "def _fmt_cell(v, floatfmt=\".4g\"):\n",
        "    try:\n",
        "        if v is None or (isinstance(v, float) and (np.isnan(v) or np.isinf(v))):\n",
        "            return str(v)\n",
        "        if isinstance(v, (int, np.integer)):\n",
        "            return str(int(v))\n",
        "        if isinstance(v, (float, np.floating)):\n",
        "            return format(float(v), floatfmt)\n",
        "        return str(v)\n",
        "    except Exception:\n",
        "        return str(v)\n",
        "\n",
        "def print_table(title: str, df: pd.DataFrame, max_rows: int = PRINT_MAX_ROWS, floatfmt: str = \".4g\"):\n",
        "    if df is None or df.empty:\n",
        "        print(f\"\\n[TABLE] {title}: (empty)\\n\")\n",
        "        return\n",
        "    df_show = df.head(max_rows).copy()\n",
        "\n",
        "    if _RICH:\n",
        "        table = Table(title=title, show_lines=True)\n",
        "        for col in df_show.columns:\n",
        "            table.add_column(str(col))\n",
        "        for _, row in df_show.iterrows():\n",
        "            table.add_row(*[_fmt_cell(v, floatfmt) for v in row.tolist()])\n",
        "        _console.print(table)\n",
        "        if len(df) > max_rows:\n",
        "            print(f\"... ({len(df) - max_rows} more rows)\")\n",
        "    else:\n",
        "        print(f\"\\n=== {title} ===\")\n",
        "        print(df_show.to_string(index=False, float_format=lambda x: format(x, floatfmt)))\n",
        "        if len(df) > max_rows:\n",
        "            print(f\"... ({len(df) - max_rows} more rows)\")\n",
        "\n",
        "def maybe_save_csv(df: pd.DataFrame, path: str, save: bool):\n",
        "    if save and (df is not None) and (not df.empty):\n",
        "        ensure_dir(os.path.dirname(path))\n",
        "        df.to_csv(path, index=False)\n",
        "\n",
        "try:\n",
        "    from einops import rearrange\n",
        "    from einops.layers.torch import Rearrange\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Please install einops: pip install einops\") from e\n",
        "\n",
        "EXP1_STEP_LOGS = []\n",
        "\n",
        "# ------------------------------\n",
        "# 0) Repro, device, I/O helpers\n",
        "# ------------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[INFO] Using device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    import torch.backends.cuda as cuda_backends\n",
        "    cuda_backends.matmul.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "def ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True); return p\n",
        "\n",
        "OUT_DIR = ensure_dir(\"./aistats_release_outputs\")\n",
        "\n",
        "# ------------------------------\n",
        "# 1) Small utilities\n",
        "# ------------------------------\n",
        "def pair(t): return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "def autocast_ctx(device, precision: str):\n",
        "    if device.type == \"cuda\":\n",
        "        dtype_map = {\"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
        "        return torch.amp.autocast(\"cuda\", enabled=(precision in dtype_map),\n",
        "                                    dtype=dtype_map.get(precision, torch.float32))\n",
        "    return nullcontext()\n",
        "\n",
        "def no_autocast_ctx(device):\n",
        "    if device.type == \"cuda\":\n",
        "        return torch.amp.autocast(\"cuda\", enabled=False)\n",
        "    return nullcontext()\n",
        "\n",
        "def bootstrap_ci(x: np.ndarray, stat_fn, n=1000, alpha=0.05, rng=None) -> Tuple[float, float, float]:\n",
        "    \"\"\"Return (stat, low, high) for the statistic on x with basic bootstrap CI.\"\"\"\n",
        "    rng = np.random.default_rng(None if rng is None else rng)\n",
        "    x = np.asarray(x); x = x[~np.isnan(x)]\n",
        "    if len(x) == 0:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    stat = stat_fn(x)\n",
        "    boots = []\n",
        "    for _ in range(n):\n",
        "        sample = rng.choice(x, size=len(x), replace=True)\n",
        "        boots.append(stat_fn(sample))\n",
        "    boots = np.sort(np.asarray(boots))\n",
        "    lo = np.percentile(boots, 100*alpha/2.0)\n",
        "    hi = np.percentile(boots, 100*(1-alpha/2.0))\n",
        "    return stat, lo, hi\n",
        "\n",
        "def loglog_regression(x, y):\n",
        "    \"\"\"\n",
        "    Simple log-log linear regression y ~ x^b -> log y = a + b log x.\n",
        "    Returns dict with slope, intercept, R2, pval (t-test on slope).\n",
        "    \"\"\"\n",
        "    x = np.asarray(x); y = np.asarray(y)\n",
        "    m = (x > 0) & (y > 0) & np.isfinite(x) & np.isfinite(y)\n",
        "    x = np.log10(x[m] + 1e-12); y = np.log10(y[m] + 1e-12)\n",
        "    if len(x) < 2:\n",
        "        return dict(n=len(x), slope=np.nan, intercept=np.nan, R2=np.nan, pval=np.nan)\n",
        "    X = np.vstack([np.ones_like(x), x]).T\n",
        "    beta, *_ = np.linalg.lstsq(X, y, rcond=None)      # [a, b]\n",
        "    yhat = X @ beta\n",
        "    resid = y - yhat\n",
        "    ss_tot = ((y - y.mean())**2).sum()\n",
        "    ss_res = (resid**2).sum()\n",
        "    R2 = 1 - ss_res / max(ss_tot, 1e-12)\n",
        "    # t-stat for slope\n",
        "    dof = max(len(x) - 2, 1)\n",
        "    s2 = ss_res / dof\n",
        "    var_beta = s2 * np.linalg.inv(X.T @ X)\n",
        "    se_b = np.sqrt(var_beta[1,1])\n",
        "    t = beta[1] / max(se_b, 1e-12)\n",
        "    from math import erf, sqrt\n",
        "    p = 2 * (1 - 0.5*(1 + erf(abs(t)/np.sqrt(2))))\n",
        "    return dict(n=len(x), slope=beta[1], intercept=beta[0], R2=R2, pval=p)\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Tiny ViT\n",
        "# ------------------------------\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, dim):\n",
        "        super().__init__()\n",
        "        H, W = pair(image_size); pH, pW = pair(patch_size)\n",
        "        assert H % pH == 0 and W % pW == 0\n",
        "        n_patches = (H // pH) * (W // pW)\n",
        "        patch_dim = 3 * pH * pW\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=pH, p2=pW),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, n_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "        x = torch.cat((self.cls_token.expand(b, -1, -1), x), dim=1)\n",
        "        x = x + self.pos_embedding[:, : n + 1]\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=64):\n",
        "        super().__init__()\n",
        "        inner = heads * dim_head\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, inner * 3, bias=False)\n",
        "        self.to_out = nn.Linear(inner, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale  # (B,H,N,N)\n",
        "        scores = dots\n",
        "        attn_probs = scores.softmax(dim=-1)\n",
        "        out = torch.matmul(attn_probs, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out), scores, attn_probs, q, k, v\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                nn.LayerNorm(dim),\n",
        "                Attention(dim, heads=heads, dim_head=dim_head),\n",
        "                nn.LayerNorm(dim),\n",
        "                nn.Sequential(nn.Linear(dim, mlp_dim), nn.GELU(), nn.Linear(mlp_dim, dim))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        inter = {\n",
        "            'scores': [], 'attn_probs': [], 'ln_pre_act': [], 'ln_modules': [],\n",
        "            'mlp_w1': [], 'mlp_w2': [], 'attn_Wo': [],      # NEW\n",
        "            'mlp_weights': [],                             # 기존 baseline 유지(여기선 W2를 넣자)\n",
        "            'q': [], 'k': [], 'v': []\n",
        "        }\n",
        "        for ln1, attn, ln2, mlp in self.layers:\n",
        "            x1 = ln1(x); inter['ln_pre_act'].append(x1); inter['ln_modules'].append(ln1)\n",
        "            attn_out, scores, attn_probs, q, k, v = attn(x1)\n",
        "            x = x + attn_out\n",
        "            x2 = ln2(x); inter['ln_pre_act'].append(x2); inter['ln_modules'].append(ln2)\n",
        "            x = x + mlp(x2)\n",
        "            inter['scores'].append(scores); inter['attn_probs'].append(attn_probs)\n",
        "            inter['q'].append(q.detach()); inter['k'].append(k.detach()); inter['v'].append(v.detach())\n",
        "            w1 = w2 = None\n",
        "            for m in mlp.modules():\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    if w1 is None: w1 = m.weight.detach()\n",
        "                    else: w2 = m.weight.detach(); break\n",
        "            inter['mlp_w1'].append(w1); inter['mlp_w2'].append(w2)\n",
        "            # baseline 용으론 W2를 사용(기존 코드 호환)\n",
        "            inter['mlp_weights'].append(w2)\n",
        "            # 어텐션 출력선형 W_O 기록\n",
        "            inter['attn_Wo'].append(attn.to_out.weight.detach())\n",
        "        return x, inter\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, dim_head=64):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding(image_size, patch_size, dim)\n",
        "        self.transformer = TransformerEncoder(dim, depth, heads, dim_head, mlp_dim)\n",
        "        self.pool = \"cls\"\n",
        "        self.mlp_head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, num_classes))\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.patch_embedding(img)\n",
        "        x, inter = self.transformer(x)\n",
        "        x = x[:, 0] if self.pool == \"cls\" else x.mean(dim=1)\n",
        "        return self.mlp_head(x), inter\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Diagnostics (paper-aligned)\n",
        "# ------------------------------\n",
        "@torch.no_grad()\n",
        "def spectral_norm_matrix_batch(mats: torch.Tensor, sample: int = None, iters: int = 7) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Spectral norm of possibly-rectangular matrices via power iteration on (M^T M).\n",
        "    mats: (..., n, d)\n",
        "    Returns: vector of spectral norms over the leading batch dims.\n",
        "    \"\"\"\n",
        "    X = mats.float()\n",
        "    if sample is not None and X.shape[0] > sample:\n",
        "        idx = torch.randperm(X.shape[0], device=X.device)[:sample]\n",
        "        X = X[idx]\n",
        "    d = X.shape[-1]\n",
        "    v = torch.randn(X.shape[:-2] + (d, 1), device=X.device)\n",
        "    v = v / (v.norm(dim=(-2, -1), keepdim=True) + 1e-12)\n",
        "    for _ in range(iters):\n",
        "        u = X.transpose(-1, -2) @ (X @ v)   # (.., d, d) @ (.., d, 1)\n",
        "        v = u / (u.norm(dim=(-2, -1), keepdim=True) + 1e-12)\n",
        "    u = X.transpose(-1, -2) @ (X @ v)\n",
        "    lam = (v.transpose(-1, -2) @ u) / (v.transpose(-1, -2) @ v + 1e-12)\n",
        "    return torch.sqrt(torch.clamp(lam.squeeze(-1).squeeze(-1), min=0)).to(torch.float32)\n",
        "\n",
        "@torch.no_grad()\n",
        "def spectral_norm_power(S: torch.Tensor, iters: int = 7) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Batched spectral norm via power iteration on S^T S.\n",
        "    S: (..., N, N)\n",
        "    returns: tensor of shape (...) with ||S||_2 estimates\n",
        "    \"\"\"\n",
        "    shape = S.shape[:-2]; N = S.shape[-1]\n",
        "    v = torch.randn(*shape, N, 1, device=S.device, dtype=S.dtype)\n",
        "    v = v / (torch.linalg.norm(v, dim=(-2, -1), keepdim=True) + 1e-12)\n",
        "    for _ in range(iters):\n",
        "        w = S @ v\n",
        "        u = S.transpose(-1, -2) @ w\n",
        "        v = u / (torch.linalg.norm(u, dim=(-2, -1), keepdim=True) + 1e-12)\n",
        "    w = S @ v\n",
        "    s2 = (v.transpose(-1, -2) @ (S.transpose(-1, -2) @ w)) / (v.transpose(-1, -2) @ v + 1e-12)\n",
        "    return torch.sqrt(torch.clamp(s2.squeeze(-1).squeeze(-1), min=0)).to(torch.float32)\n",
        "\n",
        "@torch.no_grad()\n",
        "def kappa_score_pdf(q: torch.Tensor, k: torch.Tensor, S: torch.Tensor,\n",
        "                    norm_mode: str = \"fro\",    # \"fro\" (paper body) or \"spec\" (protocols)\n",
        "                    iters: int = 7, sample_heads: int = 64) -> float:\n",
        "    B, H, N, d = q.shape\n",
        "    Q = q.reshape(B * H, N, d)\n",
        "    K = k.reshape(B * H, N, d)\n",
        "    Sm = S.reshape(B * H, N, N)\n",
        "\n",
        "    if norm_mode == \"fro\":\n",
        "        Qn = torch.linalg.norm(Q, ord='fro', dim=(1, 2))\n",
        "        Kn = torch.linalg.norm(K, ord='fro', dim=(1, 2))\n",
        "        Sn = torch.linalg.norm(Sm, ord='fro', dim=(1, 2))\n",
        "    else:\n",
        "        total = Q.shape[0]  # B*H\n",
        "        if sample_heads is not None and total > sample_heads:\n",
        "            idx = torch.randperm(total, device=Q.device)[:sample_heads]\n",
        "            Qs = Q[idx]; Ks = K[idx]; Ss = Sm[idx]\n",
        "        else:\n",
        "            Qs = Q; Ks = K; Ss = Sm\n",
        "        Qn = spectral_norm_matrix_batch(Qs, sample=None, iters=iters)\n",
        "        Kn = spectral_norm_matrix_batch(Ks, sample=None, iters=iters)\n",
        "        Sn = spectral_norm_power(Ss, iters=iters)\n",
        "\n",
        "    ratio = (Qn * Kn) / (Sn * math.sqrt(d) + 1e-12)\n",
        "    return float(torch.nan_to_num(ratio, nan=0.0, posinf=0.0).max().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def kappa_cond_W(W: torch.Tensor, ridge: float = 1e-6) -> float:\n",
        "    if W is None:\n",
        "        return float('nan')\n",
        "    s = torch.linalg.svdvals(W.float())\n",
        "    if s.numel() < 2:\n",
        "        return float('nan')\n",
        "    return float((s.max() / (s.min() + ridge)).item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def ln_C_proxy(ln_module: nn.LayerNorm, pre_act: torch.Tensor) -> float:\n",
        "    var = torch.var(pre_act, dim=-1, unbiased=False)    # per-token variance\n",
        "    sigma_med = torch.sqrt(torch.median(var)).item()\n",
        "    var_med = torch.median(var).item()\n",
        "    eps = float(max(getattr(ln_module, \"eps\", 1e-5), 1e-12))\n",
        "    return float((sigma_med / math.sqrt(eps)) + (var_med / eps) + 1.0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def kappa_softmax_pdf(attn_probs: torch.Tensor, scores: torch.Tensor, sample_rows: int = 256, iters: int = 7) -> float:\n",
        "    P = attn_probs.detach().to(torch.float32)\n",
        "    S = scores.detach().to(torch.float32)\n",
        "    B, H, N, _ = P.shape\n",
        "    P_rows = P.reshape(B*H*N, N)\n",
        "    S_rows = S.reshape(B*H*N, N)\n",
        "    M = P_rows.size(0)\n",
        "    if M > sample_rows:\n",
        "        idx = torch.randperm(M, device=P_rows.device)[:sample_rows]\n",
        "        P_rows = P_rows[idx]; S_rows = S_rows[idx]\n",
        "    v = torch.randn_like(P_rows)\n",
        "    v = v / (v.norm(dim=-1, keepdim=True) + 1e-12)\n",
        "    for _ in range(iters):\n",
        "        pv = (P_rows * v).sum(dim=-1, keepdim=True)\n",
        "        Jv = P_rows * v - pv * P_rows\n",
        "        v = Jv / (Jv.norm(dim=-1, keepdim=True) + 1e-12)\n",
        "    pv = (P_rows * v).sum(dim=-1, keepdim=True)\n",
        "    Jv = P_rows * v - pv * P_rows\n",
        "    lam = (v * Jv).sum(dim=-1) / (v * v).sum(dim=-1).clamp_min(1e-12)\n",
        "    scale = (S_rows.norm(dim=-1) / (P_rows.norm(dim=-1) + 1e-12))\n",
        "    val = (lam.clamp(min=0) * scale)\n",
        "    return float(val.max().item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def kappa_V_pdf(v: torch.Tensor, ridge: float = 1e-6, sample_heads: int = 32) -> float:\n",
        "    B, H, N, d = v.shape\n",
        "    total = B*H\n",
        "    v_mat = v.reshape(total, N, d).float()\n",
        "    if total > sample_heads:\n",
        "        idx = torch.randperm(total, device=v_mat.device)[:sample_heads]\n",
        "        v_mat = v_mat[idx]\n",
        "    vals = []\n",
        "    for Vm in v_mat:\n",
        "        try:\n",
        "            s = torch.linalg.svdvals(Vm)\n",
        "            if s.numel() < 2: continue\n",
        "            kappa = (s.max() / (s.min() + ridge)).item()\n",
        "            vals.append(kappa)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return float(np.nanmax(vals) if vals else np.nan)\n",
        "\n",
        "@torch.no_grad()\n",
        "def attention_entropy(attn_probs, sample_rows=256):\n",
        "    p = attn_probs.detach().to(torch.float32)\n",
        "    B, H, T, _ = p.shape\n",
        "    rows = p.reshape(B*H*T, T)\n",
        "    if rows.size(0) > sample_rows:\n",
        "        idx = torch.randperm(rows.size(0), device=rows.device)[:sample_rows]\n",
        "        rows = rows[idx]\n",
        "    eps = 1e-12\n",
        "    ent = -(rows * (rows+eps).log()).sum(dim=-1)\n",
        "    return ent.mean().item()\n",
        "\n",
        "def _eps_mach_for(precision: str) -> float:\n",
        "    try:\n",
        "        if precision == \"fp16\":\n",
        "            return float(torch.finfo(torch.float16).eps)\n",
        "        if precision == \"bf16\":\n",
        "            return float(torch.finfo(torch.bfloat16).eps)\n",
        "        if precision == \"fp32\":\n",
        "            return float(torch.finfo(torch.float32).eps)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # fallback\n",
        "    return float(torch.finfo(torch.float32).eps)\n",
        "\n",
        "@torch.no_grad()\n",
        "def rho_LN(ln_module: nn.LayerNorm, pre_act: torch.Tensor, eps_mach: float = None) -> float:\n",
        "    if eps_mach is None:\n",
        "        eps_mach = float(torch.finfo(torch.float32).eps)\n",
        "    var = torch.var(pre_act, dim=-1, unbiased=False)\n",
        "    sigma2_med = torch.median(var).item()\n",
        "    d_model = pre_act.shape[-1]\n",
        "    return float((sigma2_med / max(getattr(ln_module, \"eps\", 1e-12), 1e-12)) * d_model * eps_mach)\n",
        "\n",
        "@torch.no_grad()\n",
        "def forward_error_same_weights(model, x, lp_precision=\"fp16\"):\n",
        "    with no_autocast_ctx(device):\n",
        "        y_ref, _ = model(x)\n",
        "    dtype_map = {\"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
        "    with torch.amp.autocast(\"cuda\", enabled=(lp_precision in dtype_map),\n",
        "                            dtype=dtype_map.get(lp_precision, torch.float16)):\n",
        "        y_lp, _ = model(x)\n",
        "    num = torch.norm(y_lp.float() - y_ref.float())\n",
        "    den = torch.norm(y_ref.float()) + 1e-12\n",
        "    return (num/den).item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def weight_condition_number(weights: List[torch.Tensor]):\n",
        "    if len(weights) == 0:\n",
        "        return np.nan\n",
        "    W = weights[-1].float()\n",
        "    try:\n",
        "        s = torch.linalg.svdvals(W)\n",
        "        s = s[s>0]\n",
        "        if s.numel() < 2: return np.nan\n",
        "        return (s.max()/s.min()).item()\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "@torch.no_grad()\n",
        "def spectral_norm_W(W: torch.Tensor) -> float:\n",
        "    if W is None:\n",
        "        return float('nan')\n",
        "    s = torch.linalg.svdvals(W.float())\n",
        "    return float(s.max().item()) if s.numel() else float('nan')\n",
        "\n",
        "# ------------------------------\n",
        "# 4) Data & training\n",
        "# ------------------------------\n",
        "def build_dataloader(batch_size=256, num_workers=None, generator=None):\n",
        "    if num_workers is None: num_workers = os.cpu_count() or 4\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ])\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    return DataLoader(\n",
        "        trainset, batch_size=batch_size, shuffle=True, drop_last=True,\n",
        "        num_workers=num_workers, pin_memory=True, persistent_workers=True, prefetch_factor=4,\n",
        "        generator=generator\n",
        "    )\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion,\n",
        "                    precision=\"fp16\", log_freq=20, max_steps=60,\n",
        "                    ksoft_rows=256, kscore_mats=64, power_iters=7,\n",
        "                    norm_mode=\"spec\",\n",
        "                    use_res_relax=False):\n",
        "    model.train()\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(precision==\"fp16\" and device.type==\"cuda\"))\n",
        "    logs = []\n",
        "\n",
        "    max_steps = min(max_steps, len(dataloader))\n",
        "    pbar = tqdm(dataloader, total=max_steps, desc=f\"Train(prec={precision})\")\n",
        "\n",
        "    for step, (inputs, labels) in enumerate(pbar):\n",
        "        if step >= max_steps:\n",
        "            break\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast_ctx(device, precision):\n",
        "            outputs, inter = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            logs.append({'step': step, 'loss': float('nan'), 'precision': precision, 'failure': 1})\n",
        "            break\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if (step % log_freq) == 0:\n",
        "            entry = {'step': step, 'loss': float(loss.detach().cpu()),\n",
        "                     'precision': precision}\n",
        "\n",
        "            L = len(inter['scores'])\n",
        "            per_layer_terms = []\n",
        "            rho_factors = []\n",
        "            ksoft_all, kscore_all, kV_all = [], [], []\n",
        "\n",
        "            attn_core_sum = 0.0\n",
        "            attn_term_sum = 0.0\n",
        "            k_eff_sum     = 0.0\n",
        "            C_LN_sum      = 0.0\n",
        "            for li in range(L):\n",
        "                scores = inter['scores'][li]\n",
        "                probs  = inter['attn_probs'][li]\n",
        "                q      = inter['q'][li]\n",
        "                k      = inter['k'][li]\n",
        "                v      = inter['v'][li]\n",
        "                Wo     = inter['attn_Wo'][li]\n",
        "                W1     = inter['mlp_w1'][li]\n",
        "                W2     = inter['mlp_w2'][li]\n",
        "\n",
        "                # --- Paper-aligned diagnostics ---\n",
        "                ksoft = kappa_softmax_pdf(probs, scores, sample_rows=ksoft_rows, iters=power_iters)\n",
        "                kscore = kappa_score_pdf(q, k, scores, norm_mode=norm_mode, iters=power_iters, sample_heads=kscore_mats)\n",
        "                kV = kappa_V_pdf(v, ridge=3e-6, sample_heads=kscore_mats)\n",
        "\n",
        "                Wo_spec = spectral_norm_W(Wo)\n",
        "                attn_core = ksoft * (1.0 + kscore) * kV\n",
        "                attn_term = attn_core * Wo_spec\n",
        "\n",
        "                k_eff = kappa_cond_W(W1) + kappa_cond_W(W2) + kappa_cond_W(Wo)\n",
        "\n",
        "                ln_idx_a = 2*li\n",
        "                ln_idx_b = 2*li + 1\n",
        "                try:\n",
        "                    C_LN_a = ln_C_proxy(inter['ln_modules'][ln_idx_a], inter['ln_pre_act'][ln_idx_a])\n",
        "                except Exception:\n",
        "                    C_LN_a = 0.0\n",
        "                try:\n",
        "                    C_LN_b = ln_C_proxy(inter['ln_modules'][ln_idx_b], inter['ln_pre_act'][ln_idx_b])\n",
        "                except Exception:\n",
        "                    C_LN_b = 0.0\n",
        "                C_LN = max(C_LN_a, C_LN_b)\n",
        "\n",
        "                term = attn_term + k_eff + C_LN\n",
        "                per_layer_terms.append(term)\n",
        "\n",
        "                rho_hat = min(0.9, Wo_spec + spectral_norm_W(W2) * spectral_norm_W(W1))\n",
        "                rho_factors.append(1.0 + (rho_hat if use_res_relax else 0.0))\n",
        "\n",
        "                ksoft_all.append(ksoft); kscore_all.append(kscore); kV_all.append(kV)\n",
        "\n",
        "                attn_core_sum += float(attn_core)\n",
        "                attn_term_sum += float(attn_term)\n",
        "                k_eff_sum     += float(k_eff)\n",
        "                C_LN_sum      += float(C_LN)\n",
        "\n",
        "            if use_res_relax:\n",
        "                pred_sum = 0.0\n",
        "                for li, t in enumerate(per_layer_terms):\n",
        "                    down = 1.0\n",
        "                    if li + 1 < L:\n",
        "                        for rr in rho_factors[li+1:]:\n",
        "                            down *= rr\n",
        "                    pred_sum += t * down\n",
        "            else:\n",
        "                pred_sum = float(np.sum(per_layer_terms))\n",
        "\n",
        "            entry['k_softmax_pdf'] = float(np.max(ksoft_all))\n",
        "            entry['k_score_pdf']   = float(np.max(kscore_all))\n",
        "            entry['k_V_pdf']       = float(np.max(kV_all))\n",
        "            entry['pred_rhs']      = float(pred_sum)\n",
        "\n",
        "            entry['attn_core_sum'] = float(attn_core_sum)\n",
        "            entry['attn_term_sum'] = float(attn_term_sum)\n",
        "            entry['k_eff_sum']     = float(k_eff_sum)\n",
        "            entry['C_LN_sum']      = float(C_LN_sum)\n",
        "\n",
        "            # baselines\n",
        "            entry['entropy'] = float(np.mean([\n",
        "                attention_entropy(inter['attn_probs'][li], sample_rows=min(4*ksoft_rows, 2048))\n",
        "                for li in range(L)\n",
        "            ]))\n",
        "            entry['w_cond']  = weight_condition_number(inter['mlp_weights'])\n",
        "\n",
        "            try:\n",
        "                ln_mod_last = inter['ln_modules'][-1]\n",
        "                pre_last    = inter['ln_pre_act'][-1].detach()\n",
        "                entry['rho_LN'] = rho_LN(ln_mod_last, pre_last)\n",
        "            except Exception:\n",
        "                entry['rho_LN'] = float('nan')\n",
        "\n",
        "            entry['fwd_error'] = forward_error_same_weights(model, inputs, lp_precision=precision)\n",
        "            entry['failure'] = 0\n",
        "            logs.append(entry)\n",
        "            pbar.set_postfix({'loss': entry['loss'], 'k_softmax_pdf': entry['k_softmax_pdf']})\n",
        "\n",
        "    return pd.DataFrame(logs)\n",
        "\n",
        "# ------------------------------\n",
        "# 5) Experiments\n",
        "# ------------------------------\n",
        "@dataclass\n",
        "class ExpConfig:\n",
        "    seeds: List[int]\n",
        "    precision: List[str]\n",
        "    dims: List[int]\n",
        "    image_size: int = 32\n",
        "    patch: int = 4\n",
        "    heads: int = 4\n",
        "    depth: int = 2\n",
        "    iters_exp1: int = 7\n",
        "    iters_exp2: int = 7\n",
        "    iters_exp3: int = 7\n",
        "    steps_exp1: int = 160\n",
        "    steps_exp2: int = 800\n",
        "    steps_exp3: int = 400\n",
        "    ksoft_rows: int = 256\n",
        "    kscore_mats: int = 64\n",
        "    batch_size: int = 256\n",
        "\n",
        "def run_exp1_decisive(dataloader, cfg: ExpConfig, save_csv: bool = False, print_tables: bool = True):\n",
        "    rows = []\n",
        "    for seed in cfg.seeds:\n",
        "        set_seed(seed)\n",
        "        for prec in cfg.precision:\n",
        "            for dim in cfg.dims:\n",
        "                print(f\"[Exp1] seed={seed} prec={prec} dim={dim}\")\n",
        "                model = ViT(image_size=cfg.image_size, patch_size=cfg.patch, num_classes=10,\n",
        "                            dim=dim, depth=cfg.depth, heads=cfg.heads, mlp_dim=dim*2).to(device)\n",
        "                opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "                crit = nn.CrossEntropyLoss()\n",
        "                df = train_one_epoch(\n",
        "                    model, dataloader, opt, crit,\n",
        "                    precision=prec, max_steps=cfg.steps_exp1,\n",
        "                    ksoft_rows=cfg.ksoft_rows, kscore_mats=cfg.kscore_mats,\n",
        "                    power_iters=cfg.iters_exp1, norm_mode=\"spec\"\n",
        "                )\n",
        "                if df.empty:\n",
        "                    continue\n",
        "                df['seed'] = seed\n",
        "                df['precision'] = prec\n",
        "                df['dim'] = dim\n",
        "                EXP1_STEP_LOGS.append(df.copy())\n",
        "\n",
        "                q95 = lambda s: float(np.nanquantile(s, 0.95))\n",
        "                rows.append(dict(\n",
        "                    seed=seed, precision=prec, dim=dim,\n",
        "                    pred_combo   = q95(df['pred_rhs']),\n",
        "                    pred_ksoft   = q95(df['k_softmax_pdf']),\n",
        "                    pred_kscore  = q95(df['k_score_pdf']),\n",
        "                    pred_kV      = q95(df['k_V_pdf']),\n",
        "                    pred_entropy = q95(df['entropy']),\n",
        "                    pred_wcond   = float(np.nanmedian(df['w_cond'])),\n",
        "                    obs_error    = float(np.nanquantile(df['fwd_error'], 0.95)),\n",
        "                    failure_rate = float(df['failure'].mean())\n",
        "                ))\n",
        "\n",
        "    res = pd.DataFrame(rows)\n",
        "    if res.empty:\n",
        "        print(\"[Exp1] No results.\")\n",
        "        return res, pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    agg_keys = ['precision', 'dim']\n",
        "    agg = res.groupby(agg_keys).agg({\n",
        "        'pred_combo':   ['mean', 'std'],\n",
        "        'pred_ksoft':   ['mean', 'std'],\n",
        "        'pred_kscore':  ['mean', 'std'],\n",
        "        'pred_kV':      ['mean', 'std'],\n",
        "        'pred_entropy': ['mean', 'std'],\n",
        "        'pred_wcond':   ['mean', 'std'],\n",
        "        'obs_error':    ['mean', 'std'],\n",
        "        'failure_rate': ['mean']\n",
        "    }).reset_index()\n",
        "\n",
        "    flat_cols = []\n",
        "    for c in agg.columns:\n",
        "        if isinstance(c, tuple):\n",
        "            base, stat = c\n",
        "            flat_cols.append(f\"{base}_{stat}\" if stat else base)\n",
        "        else:\n",
        "            flat_cols.append(c)\n",
        "    agg.columns = flat_cols\n",
        "\n",
        "    maybe_save_csv(agg, os.path.join(OUT_DIR, \"exp1_results_by_config.csv\"), save_csv)\n",
        "    maybe_save_csv(res, os.path.join(OUT_DIR, \"exp1_results_raw.csv\"), save_csv)\n",
        "    if print_tables:\n",
        "        print_table(\"Exp1 — results by config (aggregated)\", agg)\n",
        "        print_table(\"Exp1 — results raw (per seed/prec/dim, q95 summaries)\", res)\n",
        "\n",
        "    lp = agg[agg['precision'].isin(['fp16', 'bf16'])].copy()\n",
        "    if lp.empty:\n",
        "        lp = agg.copy()\n",
        "\n",
        "    proxies = [\n",
        "        ('pred_combo_mean',  'Combined (ours)'),\n",
        "        ('pred_ksoft_mean',  'k_softmax'),\n",
        "        ('pred_kscore_mean', 'k_score'),\n",
        "        ('pred_kV_mean',     'k(V)'),\n",
        "        ('pred_entropy_mean','Entropy'),\n",
        "        ('pred_wcond_mean',  'WeightCond')\n",
        "    ]\n",
        "\n",
        "    stats_rows = []\n",
        "    for col, label in proxies:\n",
        "        if col not in lp.columns:\n",
        "            continue\n",
        "        x = lp[col].to_numpy()\n",
        "        y = lp['obs_error_mean'].to_numpy()\n",
        "        if np.sum(np.isfinite(x) & np.isfinite(y)) < 3:\n",
        "            stats_rows.append(dict(proxy=label, pearson_log=np.nan, spearman=np.nan,\n",
        "                                     n=len(x), slope=np.nan, intercept=np.nan, R2=np.nan, pval=np.nan))\n",
        "            continue\n",
        "        pearson = np.corrcoef(np.log10(x + 1e-12), np.log10(y + 1e-12))[0, 1]\n",
        "        rx = np.argsort(np.argsort(x)); ry = np.argsort(np.argsort(y))\n",
        "        spearman = np.corrcoef(rx, ry)[0, 1]\n",
        "        reg = loglog_regression(x, y)\n",
        "        stats_rows.append(dict(proxy=label, pearson_log=pearson, spearman=spearman, **reg))\n",
        "    stats = pd.DataFrame(stats_rows)\n",
        "\n",
        "    maybe_save_csv(stats, os.path.join(OUT_DIR, \"exp1_proxy_stats.csv\"), save_csv)\n",
        "    if print_tables:\n",
        "        print_table(\"Exp1 — proxy stats (LP mean vs observed mean)\", stats)\n",
        "\n",
        "    def eps_of(prec: str) -> float:\n",
        "        try:\n",
        "            if prec == 'fp16':   return float(torch.finfo(torch.float16).eps)\n",
        "            if prec == 'bf16':   return float(torch.finfo(torch.bfloat16).eps)\n",
        "            if prec == 'fp32':   return float(torch.finfo(torch.float32).eps)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return {'fp16': 2**-10, 'bf16': 2**-7}.get(prec, np.finfo(np.float32).eps)\n",
        "\n",
        "    res['eps_mach'] = res['precision'].map(eps_of)\n",
        "    res['pred_combo_eps'] = res['pred_combo'] * res['eps_mach']\n",
        "    maybe_save_csv(res, os.path.join(OUT_DIR, \"exp1_results_raw_with_eps.csv\"), save_csv)\n",
        "\n",
        "    def compute_stats(df, ycol):\n",
        "        rows_s = []\n",
        "        proxies2 = [\n",
        "            ('pred_combo',     'Combined (ours)'),\n",
        "            ('pred_combo_eps', 'Combined·εmach'),\n",
        "            ('pred_ksoft',     'k_softmax'),\n",
        "            ('pred_kscore',    'k_score'),\n",
        "            ('pred_kV',        'k(V)'),\n",
        "            ('pred_entropy',   'Entropy'),\n",
        "            ('pred_wcond',     'WeightCond'),\n",
        "        ]\n",
        "        keep = [c for c,_ in proxies2 if c in df.columns] + [ycol]\n",
        "        df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=keep, how='any')\n",
        "        npoints = len(df)\n",
        "        for key, lab in proxies2:\n",
        "            if key not in df.columns:\n",
        "                continue\n",
        "            x = df[key].to_numpy()\n",
        "            y = df[ycol].to_numpy()\n",
        "            if np.sum(np.isfinite(x) & np.isfinite(y)) < 3:\n",
        "                rows_s.append(dict(proxy=lab, pearson_log=np.nan, spearman=np.nan,\n",
        "                                     n=npoints, slope=np.nan, intercept=np.nan, R2=np.nan, pval=np.nan))\n",
        "                continue\n",
        "            pearson = np.corrcoef(np.log10(x + 1e-12), np.log10(y + 1e-12))[0, 1]\n",
        "            rx = np.argsort(np.argsort(x)); ry = np.argsort(np.argsort(y))\n",
        "            spearman = np.corrcoef(rx, ry)[0, 1]\n",
        "            reg = loglog_regression(x, y)\n",
        "            rows_s.append(dict(proxy=lab, pearson_log=pearson, spearman=spearman, **reg))\n",
        "        return pd.DataFrame(rows_s)\n",
        "\n",
        "    stats_by_prec = []\n",
        "    for prec in ['fp16','bf16']:\n",
        "        sub = res[res['precision'] == prec].copy()\n",
        "        if len(sub) == 0:\n",
        "            continue\n",
        "        s = compute_stats(sub, ycol='obs_error')\n",
        "        s.insert(0, 'precision', prec)\n",
        "        stats_by_prec.append(s)\n",
        "    stats_by_prec = pd.concat(stats_by_prec, ignore_index=True) if stats_by_prec else pd.DataFrame()\n",
        "    maybe_save_csv(stats_by_prec, os.path.join(OUT_DIR, \"exp1_proxy_stats_by_precision_raw.csv\"), save_csv)\n",
        "\n",
        "    stats_eps = compute_stats(res, ycol='obs_error')\n",
        "    maybe_save_csv(stats_eps, os.path.join(OUT_DIR, \"exp1_proxy_stats_mixed_with_eps.csv\"), save_csv)\n",
        "\n",
        "    if print_tables:\n",
        "        print_table(\"Exp1 — stratified proxy stats (raw, by precision)\", stats_by_prec)\n",
        "        print_table(\"Exp1 — mixed-precision proxy stats (εmach scaled)\", stats_eps)\n",
        "\n",
        "    if EXP1_STEP_LOGS:\n",
        "        step_df = pd.concat(EXP1_STEP_LOGS, ignore_index=True)\n",
        "        maybe_save_csv(step_df, os.path.join(OUT_DIR, \"exp1_step_logs.csv\"), save_csv)\n",
        "\n",
        "        cols = [\"seed\",\"precision\",\"dim\",\"step\",\"pred_rhs\",\"k_softmax_pdf\",\"k_score_pdf\",\"k_V_pdf\",\"fwd_error\"]\n",
        "        step_df = step_df[cols].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        def winsorize(s):\n",
        "            lo, hi = np.nanpercentile(s, [1, 99])\n",
        "            return np.clip(s, lo, hi)\n",
        "\n",
        "        for c in [\"pred_rhs\",\"k_softmax_pdf\",\"k_score_pdf\",\"k_V_pdf\",\"fwd_error\"]:\n",
        "            step_df[c] = winsorize(step_df[c])\n",
        "\n",
        "        out_rows = []\n",
        "        for prec in [\"fp16\",\"bf16\",\"fp32\"]:\n",
        "            sub = step_df[step_df[\"precision\"]==prec].copy()\n",
        "            if len(sub) < 100:\n",
        "                continue\n",
        "            st = loglog_regression(sub[\"pred_rhs\"].to_numpy(), sub[\"fwd_error\"].to_numpy())\n",
        "            sp = pd.Series(np.log10(sub[\"pred_rhs\"]+1e-12)).corr(\n",
        "                 pd.Series(np.log10(sub[\"fwd_error\"]+1e-12)), method=\"spearman\")\n",
        "            kd = pd.Series(np.log10(sub[\"pred_rhs\"]+1e-12)).corr(\n",
        "                 pd.Series(np.log10(sub[\"fwd_error\"]+1e-12)), method=\"kendall\")\n",
        "            out_rows.append(dict(precision=prec, n=len(sub), spearman=sp, kendall=kd, **st))\n",
        "        pointwise_by_prec = pd.DataFrame(out_rows)\n",
        "        maybe_save_csv(pointwise_by_prec, os.path.join(OUT_DIR,\"exp1_pointwise_stats_by_precision.csv\"), save_csv)\n",
        "        if print_tables:\n",
        "            print_table(\"Exp1 — pointwise stats by precision (winsorized 1/99%)\", pointwise_by_prec)\n",
        "\n",
        "    lp2 = agg[agg['precision'].isin(['fp16', 'bf16'])].copy()\n",
        "    if lp2.empty:\n",
        "        lp2 = agg.copy()\n",
        "\n",
        "    if 'pred_combo_mean' in lp2.columns:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.scatterplot(\n",
        "            data=lp2, x='pred_combo_mean', y='obs_error_mean',\n",
        "            hue='precision', style='dim', s=160, alpha=0.9\n",
        "        )\n",
        "        plt.xscale('log'); plt.yscale('log')\n",
        "        plt.title('Exp1: Predicted vs Observed Instability (LP only, mean over seeds)')\n",
        "        plt.xlabel('Predicted RHS: k_softmax · (1 + k_score) · k(V) · ||W_O||_2  +  κ_eff  +  C_LN')\n",
        "        plt.ylabel('Observed FP32 vs LP error')\n",
        "        plt.grid(True, which='both', ls='--', alpha=0.5)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUT_DIR, \"fig_exp1.png\"), dpi=240)\n",
        "        plt.close()\n",
        "\n",
        "    raw_lp = res[res['precision'].isin(['fp16','bf16'])].copy()\n",
        "    if len(raw_lp) >= 3:\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.scatterplot(data=raw_lp, x='pred_combo', y='obs_error',\n",
        "                        hue='precision', style='dim', size='seed', sizes=(60,160), alpha=0.9)\n",
        "        plt.xscale('log'); plt.yscale('log')\n",
        "        plt.title('Exp1 Raw: Predicted vs Observed (per-seed)')\n",
        "        plt.xlabel('Predicted RHS: k_softmax · (1 + k_score) · k(V) · ||W_O||_2  +  κ_eff  +  C_LN')\n",
        "        plt.ylabel('Observed FP32 vs LP error')\n",
        "        plt.grid(True, which='both', ls='--', alpha=0.5)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUT_DIR, \"fig_exp1_raw_scatter.png\"), dpi=240)\n",
        "        plt.close()\n",
        "\n",
        "    if len(res) >= 3:\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.scatterplot(data=res, x='pred_combo_eps', y='obs_error',\n",
        "                        hue='precision', style='dim', size='seed', sizes=(60,160), alpha=0.9)\n",
        "        plt.xscale('log'); plt.yscale('log')\n",
        "        plt.title('Exp1 Mixed: Predicted·εmach vs Observed (per-seed, mixed precision)')\n",
        "        plt.xlabel('Predicted RHS: k_softmax · (1 + k_score) · k(V) · ||W_O||_2  +  κ_eff  +  C_LN')\n",
        "        plt.ylabel('Observed FP32 vs LP error')\n",
        "        plt.grid(True, which='both', ls='--', alpha=0.5)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUT_DIR, \"fig_exp1_mixed_eps_scatter.png\"), dpi=240)\n",
        "        plt.close()\n",
        "\n",
        "    return res, agg, stats\n",
        "\n",
        "def posthoc_pointwise_analysis(step_csv_path: str):\n",
        "    import os\n",
        "    if not os.path.exists(step_csv_path):\n",
        "        print(f\"[Exp1] step csv not found: {step_csv_path}\")\n",
        "        return\n",
        "    df = pd.read_csv(step_csv_path)\n",
        "\n",
        "    cols = [\"seed\",\"precision\",\"dim\",\"step\",\"pred_rhs\",\"k_softmax_pdf\",\"k_score_pdf\",\"k_V_pdf\",\"fwd_error\"]\n",
        "    df = df[cols].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "    def winsorize(s):\n",
        "        lo, hi = np.nanpercentile(s, [1, 99])\n",
        "        return np.clip(s, lo, hi)\n",
        "    for c in [\"pred_rhs\",\"k_softmax_pdf\",\"k_score_pdf\",\"k_V_pdf\",\"fwd_error\"]:\n",
        "        df[c] = winsorize(df[c])\n",
        "\n",
        "    out_rows = []\n",
        "    for prec in [\"fp16\",\"bf16\",\"fp32\"]:\n",
        "        sub = df[df[\"precision\"]==prec].copy()\n",
        "        if len(sub) < 100:\n",
        "            continue\n",
        "        X = np.log10(sub[\"pred_rhs\"] + 1e-12)\n",
        "        Y = np.log10(sub[\"fwd_error\"] + 1e-12)\n",
        "        sp = pd.Series(X).corr(pd.Series(Y), method=\"spearman\")\n",
        "        kd = pd.Series(X).corr(pd.Series(Y), method=\"kendall\")\n",
        "        st = loglog_regression(sub[\"pred_rhs\"].to_numpy(), sub[\"fwd_error\"].to_numpy())\n",
        "        out_rows.append(dict(precision=prec, n=len(sub), spearman=sp, kendall=kd, **st))\n",
        "    pd.DataFrame(out_rows).to_csv(os.path.join(OUT_DIR,\"exp1_pointwise_stats_by_precision.csv\"), index=False)\n",
        "\n",
        "    rows = []\n",
        "    for (prec,dim,seed), g in df.groupby([\"precision\",\"dim\",\"seed\"]):\n",
        "        if len(g) < 30:\n",
        "            continue\n",
        "        st = loglog_regression(g[\"pred_rhs\"].to_numpy(), g[\"fwd_error\"].to_numpy())\n",
        "        rows.append(dict(precision=prec, dim=dim, seed=seed, **st))\n",
        "    perrun = pd.DataFrame(rows)\n",
        "    perrun.to_csv(os.path.join(OUT_DIR,\"exp1_pointwise_slope_per_run.csv\"), index=False)\n",
        "\n",
        "    for prec in [\"fp16\",\"bf16\"]:\n",
        "        sub = df[df[\"precision\"]==prec]\n",
        "        if len(sub) < 200:\n",
        "            continue\n",
        "        plt.figure(figsize=(7.2,5.8))\n",
        "        plt.hexbin(sub[\"pred_rhs\"], sub[\"fwd_error\"], gridsize=40, xscale=\"log\", yscale=\"log\", mincnt=3)\n",
        "        plt.xlabel(\"Pred RHS (k_softmax · (1 + k_score) · k(V))\")\n",
        "        plt.ylabel(\"Observed LP vs FP32 error\")\n",
        "        plt.title(f\"Exp1 Pointwise (precision={prec})\")\n",
        "        plt.colorbar(label=\"counts\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUT_DIR, f\"fig_exp1_pointwise_hex_{prec}.png\"), dpi=220)\n",
        "        plt.close()\n",
        "\n",
        "def _exp2_analyze_one(df, seed, target, out_tag, OUT_DIR):\n",
        "    \"\"\"\n",
        "    동일한 조기경보 분석을 target(= 'loss' 또는 'fwd_error')에 대해 수행:\n",
        "      - 타임시리즈 도표 저장\n",
        "      - lead–lag 및 permutation test\n",
        "      - Spike Precision@K\n",
        "    out_tag는 파일명 구분용 접미사(예: 'loss', 'fwd').\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(12,5))\n",
        "    ax1.plot(df['step'], df[target], lw=1.8, label=target)\n",
        "    ax1.set_xlabel('Step'); ax1.set_ylabel(target)\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(df['step'], df['k_softmax_pdf'], lw=1.8, label='max k_softmax (paper)')\n",
        "    ax2.set_ylabel('max k_softmax (paper)')\n",
        "    fig.suptitle(f'Exp2: Early Warning (seed={seed}, target={target})')\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(OUT_DIR, f\"fig_exp2_seed{seed}_{out_tag}.png\"), dpi=220)\n",
        "    plt.close(fig)\n",
        "\n",
        "    x = df['k_softmax_pdf'].to_numpy()\n",
        "    y = df[target].to_numpy()\n",
        "    x = (x - x.mean())/(x.std()+1e-12); y = (y - y.mean())/(y.std()+1e-12)\n",
        "\n",
        "    max_lag = min(60, len(x)//3)\n",
        "    lags = np.arange(-max_lag, max_lag+1)\n",
        "    ccs=[]\n",
        "    for L in lags:\n",
        "        if L<0: ccs.append(np.corrcoef(x[-L:], y[:len(y)+L])[0,1])\n",
        "        elif L>0: ccs.append(np.corrcoef(x[:len(x)-L], y[L:])[0,1])\n",
        "        else: ccs.append(np.corrcoef(x,y)[0,1])\n",
        "    ccs = np.array(ccs)\n",
        "    best_idx = int(np.nanargmax(ccs))\n",
        "    best_lag = int(lags[best_idx])\n",
        "    best_cc  = float(ccs[best_idx])\n",
        "\n",
        "    rng = np.random.default_rng(seed+123)\n",
        "    perm = []\n",
        "    for _ in range(1000):\n",
        "        px = rng.permutation(x)\n",
        "        ccs_p = []\n",
        "        for L in lags:\n",
        "            if L<0: ccs_p.append(np.corrcoef(px[-L:], y[:len(y)+L])[0,1])\n",
        "            elif L>0: ccs_p.append(np.corrcoef(px[:len(px)-L], y[L:])[0,1])\n",
        "            else:   ccs_p.append(np.corrcoef(px, y)[0,1])\n",
        "        perm.append(np.nanmax(ccs_p))\n",
        "    perm = np.array(perm)\n",
        "    pval = (np.sum(perm >= best_cc) + 1) / (len(perm) + 1)\n",
        "\n",
        "    plt.figure(figsize=(7,3.6))\n",
        "    plt.plot(lags, ccs)\n",
        "    plt.axvline(best_lag, ls='--')\n",
        "    plt.title(f'Lead–Lag (seed={seed}, target={target})')\n",
        "    plt.xlabel('Lag (steps)'); plt.ylabel('corr')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, f\"fig_exp2_leadlag_seed{seed}_{out_tag}.png\"), dpi=220)\n",
        "    plt.close()\n",
        "\n",
        "    H, Z, MA = 40, 1.5, 20\n",
        "    series = df[target].to_numpy()\n",
        "    mu = pd.Series(series).rolling(MA, min_periods=MA).mean().to_numpy()\n",
        "    sd = pd.Series(series).rolling(MA, min_periods=MA).std(ddof=0).to_numpy()\n",
        "    thr = (mu + Z*np.nan_to_num(sd, nan=0.0))\n",
        "    future_max = np.array([series[i:min(i+H, len(series))].max() for i in range(len(series))])\n",
        "    event = (future_max > np.maximum(series, np.nan_to_num(thr, nan=np.inf))).astype(int)\n",
        "\n",
        "    score = (df['k_softmax_pdf'] - df['k_softmax_pdf'].rolling(MA, min_periods=MA).mean()).fillna(0).to_numpy()\n",
        "    K = max(10, len(score)//10)\n",
        "    top_idx = np.argsort(score)[-K:]\n",
        "    prec_at_k = float(event[top_idx].mean())\n",
        "\n",
        "    return dict(seed=seed, target=target, best_lag=best_lag, best_corr=best_cc,\n",
        "                pval=pval, prec_at_k=prec_at_k)\n",
        "\n",
        "def run_exp2_early_warning(dataloader, cfg: ExpConfig, precision='fp16',\n",
        "                           save_csv: bool = False, print_tables: bool = True):\n",
        "    rows_per_seed = []\n",
        "    lag_rows = []\n",
        "\n",
        "    for seed in cfg.seeds:\n",
        "        set_seed(seed)\n",
        "        print(f\"[Exp2] seed={seed} prec={precision}\")\n",
        "\n",
        "        model = ViT(\n",
        "            image_size=cfg.image_size, patch_size=cfg.patch, num_classes=10,\n",
        "            dim=128, depth=cfg.depth, heads=cfg.heads, mlp_dim=256\n",
        "        ).to(device)\n",
        "        opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "\n",
        "        df = train_one_epoch(\n",
        "            model, dataloader, opt, crit,\n",
        "            precision=precision, log_freq=1, max_steps=cfg.steps_exp2,\n",
        "            ksoft_rows=cfg.ksoft_rows, kscore_mats=cfg.kscore_mats,\n",
        "            power_iters=cfg.iters_exp2\n",
        "        )\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "        df['seed'] = seed\n",
        "        rows_per_seed.append(df)\n",
        "\n",
        "        for target, tag in [('loss', 'loss'), ('fwd_error', 'fwd')]:\n",
        "            stats_one = _exp2_analyze_one(df, seed, target, tag, OUT_DIR)\n",
        "            lag_rows.append(stats_one)\n",
        "            print(\n",
        "                f\"[Exp2][seed={seed}][{target}] \"\n",
        "                f\"lag={stats_one['best_lag']}, \"\n",
        "                f\"corr={stats_one['best_corr']:.3f}, \"\n",
        "                f\"p≈{stats_one['pval']:.4f}, \"\n",
        "                f\"Prec@K≈{stats_one['prec_at_k']:.2f}\"\n",
        "            )\n",
        "\n",
        "    if not rows_per_seed:\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    df_all = pd.concat(rows_per_seed, ignore_index=True)\n",
        "    lag_df = pd.DataFrame(lag_rows)\n",
        "\n",
        "    maybe_save_csv(df_all, os.path.join(OUT_DIR, \"exp2_logs_all.csv\"), save_csv)\n",
        "    maybe_save_csv(lag_df, os.path.join(OUT_DIR, \"exp2_leadlag_both_targets.csv\"), save_csv)\n",
        "    for t, g in lag_df.groupby('target'):\n",
        "        maybe_save_csv(g, os.path.join(OUT_DIR, f\"exp2_leadlag_{t}.csv\"), save_csv)\n",
        "\n",
        "    if print_tables:\n",
        "        print_table(\"Exp2 — lead–lag & Prec@K (both targets)\", lag_df)\n",
        "\n",
        "        fwd_only = lag_df[lag_df['target'] == 'fwd_error'].copy()\n",
        "        if not fwd_only.empty:\n",
        "            summary = pd.DataFrame([{\n",
        "                'mean_lead_lag_steps': np.mean(fwd_only['best_lag']),\n",
        "                'mean_corr': np.mean(fwd_only['best_corr']),\n",
        "                'mean_Prec@K': np.mean(fwd_only['prec_at_k']),\n",
        "                'n_seeds': len(fwd_only)\n",
        "            }])\n",
        "            print_table(\"Exp2 — fwd_error summary (quick)\", summary)\n",
        "\n",
        "    return df_all, lag_df\n",
        "\n",
        "@torch.no_grad()\n",
        "def dynamic_eps_from_rhoLN(pre_act: torch.Tensor, rho_star: float,\n",
        "                           floor: float, cap: float, eps_mach: float) -> float:\n",
        "    var_per_token = torch.var(pre_act, dim=-1, unbiased=False)\n",
        "    sigma2_med = torch.median(var_per_token).item()\n",
        "    d_model = pre_act.shape[-1]\n",
        "    eps_new = (sigma2_med * d_model * eps_mach) / max(rho_star, 1e-8)\n",
        "    return float(min(max(eps_new, floor), cap))\n",
        "\n",
        "def train_with_intervention(model, dataloader, optimizer, criterion,\n",
        "                            precision=\"fp16\", intervention=False, rho_star=0.6,\n",
        "                            floor=1e-6, cap=1e-2, check_every=5, sample_for_check=16,\n",
        "                            max_steps=400, iters=7):\n",
        "    model.train()\n",
        "    original_eps = {m: m.eps for m in model.modules() if isinstance(m, nn.LayerNorm)}\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(precision==\"fp16\" and device.type==\"cuda\"))\n",
        "    logs = []\n",
        "\n",
        "    max_steps = min(max_steps, len(dataloader))\n",
        "    pbar = tqdm(dataloader, total=max_steps, desc=f\"Train(prec={precision})\")\n",
        "\n",
        "    eps_mach = _eps_mach_for(precision)\n",
        "\n",
        "    for step, (inp, lab) in enumerate(pbar):\n",
        "        if step >= max_steps:\n",
        "            break\n",
        "        inp = inp.to(device, non_blocking=True)\n",
        "        lab = lab.to(device, non_blocking=True)\n",
        "\n",
        "        if intervention and (step % check_every == 0):\n",
        "            with torch.no_grad():\n",
        "                sub = inp[:sample_for_check]\n",
        "                _, inter = model(sub)\n",
        "                for ln_mod, pre in zip(inter['ln_modules'], inter['ln_pre_act']):\n",
        "                    rho_now = rho_LN(ln_mod, pre, eps_mach=eps_mach)\n",
        "                    if rho_now < 1.0:\n",
        "                        eps_new = dynamic_eps_from_rhoLN(\n",
        "                            pre, rho_star=rho_star, floor=floor, cap=cap, eps_mach=eps_mach\n",
        "                        )\n",
        "                        ln_mod.eps = max(original_eps.get(ln_mod, ln_mod.eps), ln_mod.eps, eps_new)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast_ctx(device, precision):\n",
        "            out, _ = model(inp)\n",
        "            loss = criterion(out, lab)\n",
        "        if torch.isnan(loss):\n",
        "            loss = torch.tensor(10.0, device=device)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        try:\n",
        "            fe = forward_error_same_weights(model, inp, lp_precision=precision)\n",
        "        except Exception:\n",
        "            fe = float('nan')\n",
        "\n",
        "        logs.append({\n",
        "            'step': step,\n",
        "            'loss': float(loss.detach().cpu()),\n",
        "            'fwd_error': float(fe),\n",
        "        })\n",
        "        pbar.set_postfix({'loss': float(loss.detach().cpu()), 'fwd_error': float(fe)})\n",
        "\n",
        "    for m, eps in original_eps.items():\n",
        "        m.eps = eps\n",
        "\n",
        "    return pd.DataFrame(logs)\n",
        "\n",
        "def run_exp3_intervention(dl_control, dl_intervene, cfg: ExpConfig,\n",
        "                          precision='fp16',\n",
        "                          rho_grid=(0.5,0.6,0.7), cap_grid=(5e-3, 1e-2),\n",
        "                          save_csv: bool = False, print_tables: bool = True):\n",
        "    results = []\n",
        "    series_pngs = []\n",
        "\n",
        "    for seed in cfg.seeds:\n",
        "        set_seed(seed)\n",
        "\n",
        "        for rho_star in rho_grid:\n",
        "            for cap in cap_grid:\n",
        "                print(f\"[Exp3] seed={seed} rho*={rho_star} cap={cap}\")\n",
        "\n",
        "                # control\n",
        "                model_c = ViT(image_size=cfg.image_size, patch_size=cfg.patch, num_classes=10,\n",
        "                                dim=128, depth=cfg.depth, heads=cfg.heads, mlp_dim=256).to(device)\n",
        "                opt_c = optim.Adam(model_c.parameters(), lr=1e-3)\n",
        "                crit = nn.CrossEntropyLoss()\n",
        "                df_c = train_with_intervention(\n",
        "                    model_c, dl_control, opt_c, crit,\n",
        "                    precision=precision, intervention=False,\n",
        "                    max_steps=cfg.steps_exp3, iters=cfg.iters_exp3\n",
        "                )\n",
        "\n",
        "                # intervention\n",
        "                model_i = ViT(image_size=cfg.image_size, patch_size=cfg.patch, num_classes=10,\n",
        "                                dim=128, depth=cfg.depth, heads=cfg.heads, mlp_dim=256).to(device)\n",
        "                opt_i = optim.Adam(model_i.parameters(), lr=1e-3)\n",
        "                df_i = train_with_intervention(\n",
        "                    model_i, dl_intervene, opt_i, crit,\n",
        "                    precision=precision, intervention=True,\n",
        "                    rho_star=rho_star, cap=cap,\n",
        "                    max_steps=cfg.steps_exp3, iters=cfg.iters_exp3\n",
        "                )\n",
        "\n",
        "                # loss curves (moving average)\n",
        "                fig = plt.figure(figsize=(12,5))\n",
        "                plt.plot(df_c['step'], df_c['loss'].rolling(10).mean(), label='Control', lw=2)\n",
        "                plt.plot(df_i['step'], df_i['loss'].rolling(10).mean(),\n",
        "                         label=f'Intervene (rho*={rho_star}, cap={cap})', lw=2)\n",
        "                plt.title(f'Exp3 seed={seed}'); plt.xlabel('Step'); plt.ylabel('Loss (10-step MA)')\n",
        "                plt.legend(); plt.tight_layout()\n",
        "                png_path = os.path.join(OUT_DIR, f\"fig_exp3_seed{seed}_rho{rho_star}_cap{cap}.png\")\n",
        "                fig.savefig(png_path, dpi=220); plt.close(fig)\n",
        "                series_pngs.append(png_path)\n",
        "\n",
        "                # tail metrics\n",
        "                tail = 50\n",
        "                def tail_mean(df, key):\n",
        "                    s = df[key].tail(tail).replace([np.inf, -np.inf], np.nan).dropna()\n",
        "                    return float(s.mean()) if len(s) else float('nan')\n",
        "                m_loss_c = tail_mean(df_c, 'loss');     m_loss_i = tail_mean(df_i, 'loss')\n",
        "                m_fwd_c  = tail_mean(df_c, 'fwd_error'); m_fwd_i  = tail_mean(df_i, 'fwd_error')\n",
        "\n",
        "                results.append(dict(\n",
        "                    seed=seed, rho_star=rho_star, cap=cap,\n",
        "                    loss_tail_control=m_loss_c, loss_tail_intervene=m_loss_i,\n",
        "                    fwd_tail_control=m_fwd_c,  fwd_tail_intervene=m_fwd_i,\n",
        "                    improvement_loss=(m_loss_c - m_loss_i),\n",
        "                    improvement_fwd=(m_fwd_c - m_fwd_i)\n",
        "                ))\n",
        "\n",
        "    res = pd.DataFrame(results)\n",
        "    maybe_save_csv(res, os.path.join(OUT_DIR,\"exp3_results_grid.csv\"), save_csv)\n",
        "\n",
        "    agg = res.groupby(['rho_star','cap']).agg({\n",
        "        'improvement_loss':['mean','std'],\n",
        "        'improvement_fwd':['mean','std'],\n",
        "    }).reset_index()\n",
        "    agg.columns = ['rho_star','cap','improv_loss_mean','improv_loss_std','improv_fwd_mean','improv_fwd_std']\n",
        "    maybe_save_csv(agg, os.path.join(OUT_DIR,\"exp3_results_agg.csv\"), save_csv)\n",
        "\n",
        "    if print_tables:\n",
        "        print_table(\"Exp3 — grid (per-seed improvements)\", res)\n",
        "        print_table(\"Exp3 — aggregated improvements by (rho*, cap)\", agg)\n",
        "\n",
        "    # summary pointplots\n",
        "    avg = res.groupby(['rho_star','seed']).agg({\n",
        "        'improvement_loss':'mean',\n",
        "        'improvement_fwd':'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    plt.figure(figsize=(7,4.2))\n",
        "    sns.pointplot(data=avg, x='rho_star', y='improvement_loss', errorbar='sd')\n",
        "    plt.title('Exp3: Improvement (control - intervene) by rho*')\n",
        "    plt.ylabel('Loss improvement (↑ better)'); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR,\"fig_exp3_summary.png\"), dpi=220); plt.close()\n",
        "\n",
        "    plt.figure(figsize=(7,4.2))\n",
        "    sns.pointplot(data=avg, x='rho_star', y='improvement_fwd', errorbar='sd')\n",
        "    plt.title('Exp3: Forward-error Δ by rho*')\n",
        "    plt.ylabel('Fwd-error change (≈0 is neutral)'); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR,\"fig_exp3_summary_fwd.png\"), dpi=220); plt.close()\n",
        "\n",
        "    return res, agg, series_pngs\n",
        "\n",
        "# ------------------------------\n",
        "# 6) Main orchestrator\n",
        "# ------------------------------\n",
        "def main(fast_mode=False, run1=True, run2=False, run3=False,\n",
        "         save_csv: bool = False, print_tables: bool = True):\n",
        "    \"\"\"\n",
        "    Orchestrator\n",
        "    - save_csv: also write CSVs alongside terminal tables\n",
        "    - print_tables: pretty-print tables to terminal\n",
        "    \"\"\"\n",
        "    # ---- configs ----\n",
        "    if fast_mode:\n",
        "        cfg = ExpConfig(\n",
        "            seeds=[0, 1, 2],\n",
        "            precision=['fp32', 'fp16', 'bf16'],\n",
        "            dims=[96],\n",
        "            steps_exp1=60, steps_exp2=300, steps_exp3=200,\n",
        "            ksoft_rows=128, kscore_mats=32,\n",
        "            iters_exp1=5, iters_exp2=5, iters_exp3=5,\n",
        "            batch_size=256\n",
        "        )\n",
        "    else:\n",
        "        cfg = ExpConfig(\n",
        "            seeds=[0, 1, 2, 3, 4],\n",
        "            precision=['fp32', 'bf16', 'fp16'],\n",
        "            dims=[96, 128],\n",
        "            steps_exp1=160, steps_exp2=800, steps_exp3=500,\n",
        "            ksoft_rows=256, kscore_mats=64,\n",
        "            iters_exp1=7, iters_exp2=7, iters_exp3=7,\n",
        "            batch_size=256 if device.type == \"cuda\" else 128\n",
        "        )\n",
        "\n",
        "    # ---- dump config ----\n",
        "    ensure_dir(OUT_DIR)\n",
        "    with open(os.path.join(OUT_DIR, \"run_config.json\"), \"w\") as f:\n",
        "        json.dump(asdict(cfg), f, indent=2)\n",
        "\n",
        "    # ---- data ----\n",
        "    trainloader = build_dataloader(batch_size=cfg.batch_size)\n",
        "\n",
        "    # ---- orchestrate ----\n",
        "    results = {}\n",
        "\n",
        "    if run1:\n",
        "        res_raw, res_cfg, stats = run_exp1_decisive(\n",
        "            trainloader, cfg, save_csv=save_csv, print_tables=print_tables\n",
        "        )\n",
        "        if save_csv:\n",
        "            results['exp1_raw'] = os.path.join(OUT_DIR, \"exp1_results_raw.csv\")\n",
        "            results['exp1_cfg'] = os.path.join(OUT_DIR, \"exp1_results_by_config.csv\")\n",
        "            results['exp1_raw_eps'] = os.path.join(OUT_DIR, \"exp1_results_raw_with_eps.csv\")\n",
        "            results['exp1_stats'] = os.path.join(OUT_DIR, \"exp1_proxy_stats.csv\")\n",
        "            results['exp1_stats_by_prec_raw'] = os.path.join(OUT_DIR, \"exp1_proxy_stats_by_precision_raw.csv\")\n",
        "            results['exp1_stats_mixed_with_eps'] = os.path.join(OUT_DIR, \"exp1_proxy_stats_mixed_with_eps.csv\")\n",
        "            results['exp1_pointwise_stats_by_precision'] = os.path.join(OUT_DIR,\"exp1_pointwise_stats_by_precision.csv\")\n",
        "            results['exp1_step_logs'] = os.path.join(OUT_DIR, \"exp1_step_logs.csv\")\n",
        "\n",
        "    if run2:\n",
        "        df_all, lag_df = run_exp2_early_warning(\n",
        "            trainloader, cfg, precision='fp16',\n",
        "            save_csv=save_csv, print_tables=print_tables\n",
        "        )\n",
        "        if save_csv:\n",
        "            results['exp2_logs'] = os.path.join(OUT_DIR, \"exp2_logs_all.csv\")\n",
        "            results['exp2_leadlag_both'] = os.path.join(OUT_DIR, \"exp2_leadlag_both_targets.csv\")\n",
        "            results['exp2_leadlag_loss'] = os.path.join(OUT_DIR, \"exp2_leadlag_loss.csv\")\n",
        "            results['exp2_leadlag_fwd'] = os.path.join(OUT_DIR, \"exp2_leadlag_fwd.csv\")\n",
        "\n",
        "    if run3:\n",
        "        print(\"[INFO] Creating dedicated dataloaders for Exp3...\")\n",
        "        g_base = torch.Generator(device='cpu'); g_base.manual_seed(42)\n",
        "        g1 = torch.Generator(device='cpu'); g1.set_state(g_base.get_state())\n",
        "        g2 = torch.Generator(device='cpu'); g2.set_state(g_base.get_state())\n",
        "        dl_control   = build_dataloader(batch_size=cfg.batch_size, generator=g1)\n",
        "        dl_intervene = build_dataloader(batch_size=cfg.batch_size, generator=g2)\n",
        "\n",
        "        res_grid, res_agg, pngs = run_exp3_intervention(\n",
        "            dl_control, dl_intervene, cfg, precision='fp16',\n",
        "            rho_grid=(0.5, 0.6, 0.7), cap_grid=(5e-3, 1e-2),\n",
        "            save_csv=save_csv, print_tables=print_tables\n",
        "        )\n",
        "        if save_csv:\n",
        "            results['exp3_grid'] = os.path.join(OUT_DIR, \"exp3_results_grid.csv\")\n",
        "            results['exp3_agg'] = os.path.join(OUT_DIR, \"exp3_results_agg.csv\")\n",
        "\n",
        "    # ---- summary ----\n",
        "    print(\"\\nSaved PNGs/CSVs in:\", OUT_DIR)\n",
        "    for k, v in results.items():\n",
        "        print(f\"  - {k}: {v}\")\n",
        "\n",
        "    print(\"\\nFigures:\")\n",
        "    figs = [\n",
        "        \"fig_exp1.png\",\n",
        "        \"fig_exp1_raw_scatter.png\",\n",
        "        \"fig_exp1_mixed_eps_scatter.png\",\n",
        "        \"fig_exp2_seed*_{loss,fwd}.png\",\n",
        "        \"fig_exp2_leadlag_seed*_{loss,fwd}.png\",\n",
        "        \"fig_exp3_seed*_rho*_cap*.png\",\n",
        "        \"fig_exp3_summary.png\",\n",
        "        \"fig_exp3_summary_fwd.png\",\n",
        "    ]\n",
        "    for f in figs:\n",
        "        print(\"  -\", f)\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Terminal tables on, CSV saving off\n",
        "    main(fast_mode=False, run1=True, run2=True, run3=True,\n",
        "         save_csv=False, print_tables=True)"
      ]
    }
  ]
}